# UDP Pong Game - Engineering Notebook

## Project Overview
For our final porejct, we implemented a multiplayer `Pong' game. Users can create accounts and play against other users selected from a queue of users currently waiting to play. The project includes user authentication, matchmaking, game lobbies, and a real-time physics simulation for the Pong game itself.

## Communication Protocol and Implementation

In the communication between the server and the clients, there is a tradeoff between ensuring every message gets received correctly and making sure messages get received quickly. In most usecases we considered in class, it was crucial that each message is sent in full with a confirmation of receipt being returned. The message rate was reasonably low and it was acceptable if the time between sending a message and the receiver fully receiving the message was on the scale of tenths of seconds. Here, however, in the context of the game, the requirements are different. Game-relevant communication between the server and the players needs to be fast -- on the scale of hundreths of seconds -- but the consequences of singular messages getting lost or messages arriving slgihtly out of order are not too high.

To make the communication in our game work efficiently and satisfy the described demand, we are using the UDP communication protocol -- instead of TCP as seen in class. This communication protocol is very common in the usecase of online games. UDP (User Datagram Protocol) consists of a client-server architecture where the server is authoritative about the game state, and clients connect to the singular server.

Since our Pong game is being player in real-time, we need to optimize for a minimal delay between player input and the visual feedback. Unlike TCP, which establishes a formal connection between sender and receiver through a three-way handshake, UDP simply sends packets without any prior setup or confirmation. In TCP, as we saw in our socket programming examples in class, the client and server must first synchronize sequence numbers and acknowledge each other before any actual data can be transmitted -- a process that typically takes three round trips and adds noticeable delay. UDP, however, operates more like dropping a letter in a mailbox -- the sender dispatches the packet and moves on without waiting for confirmation that the connection is established. This "fire-and-forget" approach means that when a player f.e. presses "up" on their keyboard, the input is immediately sent to the server without waiting for any connection formalities, resulting in more responsive gameplay. While this introduces the possibility that some packets might get lost in transit, the regularly timed game state updates ensure that any temporary inconsistencies are quickly corrected in subsequent frames -- making this tradeoff desirable for our real-time gaming application. Thus, UDP's connectionless nature eliminates the overhead of connection establishment, acknowledgments, and flow control.

When considering the specific mechanics of our implementation, we can observe how the game naturally accommodates packet loss in several ways. For instance, if a player's paddle movement input packet is lost during transmission, the resulting momentary discrepancy is quickly remedied when the next input packet arrives a fraction of a second later -- the player might notice a slight hiccup in responsiveness, but the overall gameplay continues uninterrupted. Similarly, if the server's state update packet containing ball position is lost en route to a client, the subsequent update (arriving approximately 16 milliseconds later at 60 updates per second) will correct any visual inconsistencies without meaningful gameplay impact. This inherent resilience is further enhanced by our authoritative server architecture, where the server maintains the "ground truth" of the game state, ensuring that temporary client-side deviations due to packet loss are quickly resolved. We improved this natural tolerance by implementing our tick-based game loop in `server.py`, where each frame represents a discrete advancement of game state, allowing the system to maintain consistency even when individual updates are missed. This stands in stark contrast to TCP, which would delay all subsequent updates until a lost packet is retransmitted -- creating noticeable freezes in the fast-paced gameplay environment where immediate, albeit occasionally imperfect, information is far preferable to delayed perfect information.

### Reliability mechanisms

However, we still need some methods in place to ensure reliability of the communication protocol, for example when a player disconnects. Instead of TCP's one-size-fits-all reliability approach, we use UDP since it allows us to implement reliability mechanisms particularly for our project that address the messaging reliability needs, hopefully without compromising real-time performance. 

Our reliability mechanisms begin with a heartbeat system implemented through the `Pulse` message type in `protocol.py`. This functions as a lightweight alternative to TCP's connection state, where clients automatically send pulse messages every 2 seconds to indicate their active status. The server maintains a `last_pulse_time` timestamp for each connected player, and the `_check_player_timeouts` method detects disconnections when no messages are received for 5 seconds (defined by the `PLAYER_TIMEOUT` constant). This simple mechanism achieves much of the same connection monitoring that TCP provides, but with far less overhead.

For handling network jitter and packet ordering issues, we implemented an input sequencing system. The client assigns incrementing sequence numbers to each `Input` message in `client.py`. When these arrive at the server potentially out of order, the sequence numbers allow proper ordering. Unlike TCP's approach of holding all packets until the correct sequence is established, our server processes each input as it arrives, maintaining game flow without interruption.

Perhaps the most significant reliability enhancement for player experience is our client-side prediction system implemented in the `draw()` method of the `Gui` class. When a player presses a movement key, the client immediately updates the local display to move their paddle, while simultaneously sending the input to the server. The client doesn't wait for confirmation from the server before showing this movement, creating the illusion of immediate response regardless of network latency. When authoritative state updates arrive from the server, any discrepancies between the predicted and actual positions are smoothly reconciled. This approach represents a fundamentally different philosophy than TCP's reliable delivery model - rather than ensuring perfect information at the expense of timeliness, we accept the inherently imperfect nature of real-time communication and build mechanisms that maintain gameplay integrity despite these limitations.

To establish the foundational structure of a game session without TCP's formal connection establishment, we created a lightweight Hello-Welcome handshake protocol. After authentication, the client sends a `Hello` message containing the player's username. The server validates this message, checks for duplicate usernames, and responds with a `Welcome` message that assigns a player ID (0 for left paddle, 1 for right paddle). If a `Hello` message is lost due to UDP's unreliable nature, the client has a retry mechanism that sends additional `Hello` messages if it doesn't receive a `Welcome` response within a reasonable timeframe. The server's `_handle_hello` method in `server.py` manages this process, including verification that the user is authenticated before accepting the Hello message.

The authoritative server architecture is another cornerstone of our reliability strategy. The server in the `PongServer` class maintains the definitive game state, with all physics calculations performed server-side. Through regular state broadcasts via the `broadcast_state` method, we ensure clients eventually converge to the correct state even if some update packets are lost. This approach prevents cheating while accommodating packet loss, as each new broadcast contains the complete game state rather than just incremental changes.

Timeout detection plays a crucial role in our UDP implementation, as neither end can rely on TCP's connection termination to know when the other has disconnected. On the server side, the `_check_player_timeouts` method periodically examines the `last_pulse_time` for each player. If a player hasn't sent any messages for longer than the timeout period, the server invokes `_handle_player_disconnect`, which notifies the remaining player, updates the game state, and potentially ends the game. On the client side, the `_check_server_timeout` method monitors a `pulse_from_server_time` timestamp that's updated whenever any message is received. If the client hasn't heard from the server for more than 5 seconds, it displays a "Server not responding" message and shuts down gracefully rather than leaving the user in an unresponsive state.

To handle temporary connection disruptions, we implemented an automatic reconnection system where the client stores authentication credentials. When receiving an "authentication required" message, the client automatically re-authenticates without requiring user intervention, maintaining gameplay sessions despite brief network interruptions. This is particularly important for mobile or wireless connections where signal strength may fluctuate.

Message validation provides another layer of reliability, with all received messages validated in the `decode()` function within `protocol.py`. Invalid or malformed packets are safely discarded with appropriate error logging. Protocol version checking ensures compatibility between client and server versions, preventing issues that might arise from mismatched implementations.

For network efficiency, we designed our message protocol to keep packets small, avoiding UDP fragmentation issues that can occur with oversized datagrams. While we chose JSON serialization for development practicality rather than raw binary encoding, the messages remain compact enough to avoid reliability problems related to packet size. This represents a practical tradeoff between optimal efficiency and development velocity.

Finally, our server implements a game loop with fixed timestep simulation, creating deterministic behavior regardless of packet arrival timing. This approach ensures that the physics simulation advances consistently, making it easier for clients to reconcile any discrepancies between their predicted state and the authoritative server state.

We considered several alternative protocols before settling on UDP. TCP would have introduced significant problems for our real-time game due to its head-of-line blocking behavior, where a single lost packet causes all subsequent packets to be held until retransmission occurs. TCP's congestion control algorithms can significantly reduce throughput after packet loss, potentially creating noticeable gameplay disruptions. WebSockets, while providing a convenient API and working over standard HTTP ports, ultimately operate over TCP and inherit these same limitations. QUIC is a modern protocol that offers many of UDP's benefits with built-in reliability mechanisms, but at the time of development, it lacked widespread library support and would have added implementation complexity. Custom raw sockets would have offered maximum control but would introduce cross-platform compatibility issues and potential security concerns.

## System Components and Implementation

### Network Protocol (protocol.py)
Designing a reliable communication protocol over unreliable UDP presented significant challenges. We created a JSON-based message protocol with a version system for backward compatibility and defined various message types (Hello, Pulse, Welcome, Input, State, etc.) using Python dataclasses. Message encoding/decoding was implemented with comprehensive error handling for malformed packets, and we added heartbeat mechanisms to maintain connection state and detect disconnections.

### Server Architecture (server.py)
Handling multiple concurrent games and clients efficiently required careful design. We used a multi-process architecture with a central lobby manager process that creates separate game instances for pairs of players. The timeout system we implemented detects and handles disconnected players gracefully, while our port allocation system dynamically assigns game lobbies to different ports, enabling multiple simultaneous games.

### Game Physics Simulation
Creating a deterministic physics model that works reliably over a network connection was a significant challenge. We developed a simple but effective physics engine for ball movement and collisions that uses a fixed timestep simulation to ensure consistent behavior across instances. Slight randomization in ball velocity after paddle collisions adds gameplay variety without compromising predictability, and the authoritative server-side physics implementation with client-side prediction creates a responsive UI experience despite network latency.

### User Authentication System
Securely authenticating users and maintaining persistence between sessions was implemented through a SQLite database that stores user credentials and game statistics. We used standard password hashing techniques for secure authentication and implemented robust session management to maintain authenticated state during gameplay. Statistical tracking for wins, losses, and total games played enhances the user experience and encourages continued engagement.

### Client Implementation (client.py)
Creating a responsive UI that gracefully handles network latency required several techniques. We used Pygame for rendering and input handling, implemented client-side prediction to hide network latency, and created input buffering to smooth out player control. The timeout detection system handles server disconnections gracefully, and our UI components for login, gameplay, and player information display create a cohesive user experience.

### Lobby and Matchmaking System
Efficiently pairing players together and managing the game lifecycle presented interesting challenges. We created a matchmaking queue to pair waiting players, implemented a lobby status system (WAITING, ACTIVE, COMPLETED), designed a process monitoring system to clean up completed games and prevent resource leaks, and added redirect functionality to move players from the main server to game lobbies seamlessly.

## Technical Challenges and Solutions

Despite using UDP, which doesn't guarantee packet delivery, we achieved remarkable reliability through several complementary approaches. Regular heartbeat messages detect disconnections quickly, input sequence numbers handle packet loss and out-of-order delivery gracefully, and our server-side authority with client prediction creates smooth gameplay even with significant network latency or packet loss.

Our server architecture was specifically designed for scalability through separated processes for each game (distributing CPU load), dynamic port allocation for game instances, and automatic resource cleanup for completed games to prevent memory leaks during extended operation.

Security was a primary concern, addressed through password hashing using standard cryptographic functions, thorough input validation to prevent malformed packets from causing issues, and timeout mechanisms to prevent resource exhaustion attacks.


### Error Recovery Strategy
Our error recovery approach goes beyond simply handling malformed packets. We implemented a layered recovery system where the severity of the error determines the recovery mechanism. For minor issues like a single dropped input packet, the client continues operating with its predicted state until the next server update reconciles any discrepancies. For more severe issues like a series of dropped state updates, the client implements a "dead reckoning" system that continues simulating ball movement based on its last known velocity until fresh server data arrives. If the connection deteriorates further, a progressive degradation occurs: first showing a "connection unstable" warning, then freezing the game state while attempting reconnection, and finally timing out with a graceful shutdown if recovery fails. This graduated approach prevents the jarring experience of an immediate disconnect while maximizing the chances of recovering from transient network issues.

### Data Flow Architecture
The data flow in our system follows a clear path optimized for minimum latency. When a player presses a key, the input is processed through three parallel pipelines: (1) it's immediately applied to the local prediction model for instant visual feedback, (2) it's queued in the outgoing message buffer with a sequence number, and (3) it's stored in a local history buffer for reconciliation with future server updates. When this input reaches the server, it updates the authoritative game state and then broadcasts this state to all connected clients. Upon receiving a state update, the client compares the authoritative state with its predicted state, reconciles any differences using a smooth interpolation algorithm, and then continues prediction from this new verified baseline. This architecture creates a continuous gameplay experience despite the inherent latency and unreliability of network communication.

### Concurrency Management
Managing concurrency between our lobby manager and multiple game processes required careful design to avoid race conditions. We implemented an ownership-based approach where resources are explicitly "owned" by either the lobby manager or a game process, never shared. Communication happens exclusively through message passing via pipes, with all messages treated as immutable once sent. When a player disconnects, for example, the game process has sole responsibility for updating its internal player list, while the lobby manager has exclusive control over the global authenticated users list. When these lists need to synchronize, an explicit message transfer occurs with acknowledgment, preventing any possibility of concurrent modification. The pipe-based IPC mechanism automatically handles message queuing, ensuring that even rapid-fire communications are processed in the correct order without data races.

### Performance Tuning
Performance optimization was guided by profiling rather than premature optimization. Using Python's cProfile, we identified several bottlenecks in our initial implementation. The JSON serialization/deserialization process consumed a surprising 12% of CPU time, but our benchmarks showed that switching to a binary protocol would save only 5-8ms per frame while significantly increasing development complexity. This tradeoff wasn't worth it for our latency requirements. However, we did optimize the game state broadcast by implementing a dirty-checking mechanism that only sends updates when game parameters have actually changed. This reduced network utilization by approximately 40% during periods of low activity. We also discovered that our original collision detection algorithm was needlessly precise, recalculating collisions even when objects were far apart. Implementing a basic spatial partitioning scheme reduced physics calculation time by 70% while maintaining identical gameplay behavior.

### Design Patterns
We deliberately employed several design patterns to keep our codebase maintainable. The Observer pattern forms the backbone of our state distribution system, where clients observe the server's authoritative state. We implemented a Factory pattern in the lobby manager for creating standardized game lobbies with appropriate resource allocation. The Command pattern handles network messages, encapsulating requests as objects that can be passed between processes. A modified Model-View-Controller pattern separates our game state (model) from rendering (view) and input handling (controller). Perhaps most importantly, we used the Facade pattern in our protocol implementation, hiding the complexity of error handling, retries, and packet management behind clean interfaces that the rest of the system interacts with. This pattern-based approach made our code more modular and easier to modify as requirements evolved during development.

### Deployment Considerations
While our Engineering Notebook focuses on the technical implementation, we also designed the system with real-world deployment in mind. The server component is containerized using Docker with explicit resource limits, making it deployable on any cloud provider or on-premises hardware. Network requirements are minimal: a single public-facing UDP port for the lobby manager, plus a configurable range of ephemeral ports for game instances (typically 10-50 ports depending on expected concurrent players). We implemented graceful shutdown handling to preserve game state and user sessions during server updates. The client is packaged as a standalone executable for Windows, macOS, and Linux using PyInstaller, with automatic update capability through a version check against the server. This deployment architecture allows for horizontal scaling by adding more lobby manager instances behind a UDP load balancer as player counts increase.

### Testing Automation
Our testing strategy went beyond merely writing tests - we integrated them deeply into our development workflow. We created a continuous integration pipeline using GitHub Actions that automatically runs all tests on every commit. Integration tests involving network communications run in a virtualized environment with simulated packet loss and latency to catch issues that only appear under real-world conditions. We employed mutation testing tools to verify the effectiveness of our test suite, deliberately introducing bugs to confirm they would be caught. Performance regression tests track key metrics like frame processing time and memory usage across versions, alerting us if a change negatively impacts these areas. Most crucially, we developed a replay system that records real gameplay sessions as input sequences, allowing us to deterministically reproduce and debug issues reported by users without requiring them to provide complex debugging information.

### Client Buffering Strategy
The client's state management system is more sophisticated than initially described. Rather than simply accepting each server state update as received, the client maintains a rolling buffer of recent states, typically spanning 250ms of gameplay. Incoming state packets are timestamped and inserted into this buffer in correct temporal order, regardless of arrival time. The rendering system then interpolates between the two states surrounding the current "render time" (which intentionally lags behind the latest received state by approximately 100ms). This interpolation buffer absorbs network jitter and packet loss while maintaining smooth animation. For prediction, we actually run a simplified physics simulation on the client that approximates server behavior, with periodic corrections from authoritative updates. When reconciling differences between predicted and actual states, we use a variable-rate correction that applies larger adjustments when the player isn't actively moving their paddle (when players are less likely to notice the correction), creating a seamless experience even under challenging network conditions.

## Future Improvements

Several enhancements could further improve the system. Adding a spectator mode would allow users to watch ongoing games, enhancing community engagement. Implementing NAT traversal techniques would improve connectivity for users behind restrictive firewalls. Tournament functionality could create more structured competitive play. The game physics could be enhanced with more realistic paddle physics and spin effects for more nuanced gameplay. Finally, the UI could be improved with animations and sound effects to create a more engaging sensory experience.


### Testing
We had to make sure everything worked correctly across the network, which is notoriously unpredictable. Our approach was to build tests from the ground up, starting with the basics and gradually testing more complex interactions.

We first tackled the protocol messages - making sure data packets could be properly encoded, sent across the network, and decoded correctly on the other end. This might sound straightforward, but with multiple message types (Hello, Welcome, Input, State), we needed to verify each one maintained its integrity when bouncing between client and server.

The game physics were particularly tricky to test. We had to ensure the ball bounced correctly off walls and paddles, that scoring worked correctly when the ball went past a player, and that the paddle movements responded appropriately to player input. These tests helped us catch several edge cases where the ball would occasionally behave unexpectedly after certain collisions.

Testing the server components meant simulating players connecting, sending inputs, timing out, and disconnecting. We wanted to be sure that if someone's internet dropped mid-game, the server would handle it gracefully instead of crashing or leaving the other player hanging forever.

For database testing, we created temporary test databases to verify users could register, log in, and have their game stats tracked correctly. This was essential for maintaining player progression across multiple sessions.

When we moved to integration testing, things got more interesting. We needed to ensure the authentication system properly talked to the database, that game lobbies communicated correctly with the lobby manager, and that the physics engine integrated smoothly with the networking layer. These tests revealed subtle timing issues that weren't apparent when testing components in isolation.

The communication flow tests were particularly valuable - they simulated the back-and-forth of login sequences, game initialization, and the constant stream of inputs and state updates during gameplay. These tests helped us refine our protocol to be more efficient and robust.

We spent a good amount of time on edge cases too. What happens if a player's connection drops for just a second? What if they send malformed packets? What if they try to log in twice? These scenarios happen in the real world, and our tests made sure the game wouldn't fall apart when they did.


## Attributions
The game [Pong](https://en.wikipedia.org/wiki/Pong) was developed and popularize by [Atari](https://atari.com/?srsltid=AfmBOoot0ZhkFV5EbJzfy1qfKjW8kWzW0Mg9JQTdNT9AUYnnOQHB51n8).

We used Cursor with Antropic AI to assist us with implement parts of the project and summarizing its features in this notebook.'
